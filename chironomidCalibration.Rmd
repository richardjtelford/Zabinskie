---
title: "Performance of chironomid-temperature transfer functions"
author: "Richard J. Telford"
date: "December 9, 2016"
output:
  html_document:
    keep_md: yes
bibliography: chironomid.bib
csl: journal-of-paleolimnology.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, echo = FALSE, include  = TRUE, warning = TRUE, message = FALSE)
```
```{r load}
library("ggplot2")
library("readr")
library("dplyr")
library("gridExtra")
chiron <- read_csv("chironomidCalibration.csv")
chiron <- select(chiron, -`Altitudinal gradient (m)`)
chiron <- chiron %>% mutate(year = as.numeric(gsub(".*(\\d{4}).*", "\\1", Study))) %>%
  mutate(minT = as.numeric(gsub("^(.*)–.*$", "\\1", `Tgradient (°C)`))) %>%
  mutate(maxT = as.numeric(gsub("^.*–(.*)$", "\\1", `Tgradient (°C)`))) %>%
  mutate(midT = (minT + maxT)/2) %>%
  mutate(Notes = ifelse(is.na(Notes), "", Notes))

format_p <- function(p) {
  ifelse(p < 0.001, "< 0.001", paste("=", signif(p, 2)))
}
```

The performance of transfer functions for reconstructing past temperatures from chironomid assemblages in lake sediments varies enormously. The root-mean-squared-error of prediction (RMSEP) for transfer functions in a compilation by Eggermont and Heiri [-@Eggermont2011], augmented by a few recent studies (Table 1)[^1], ranges between `r min(chiron$'RMSEP (°C)')` and `r round(max(chiron$'RMSEP (°C)'), 2)` °C. What causes this `r round(max(chiron$'RMSEP (°C)')/min(chiron$'RMSEP (°C)'), 1)`-fold difference in RMSEP? Understanding this might lead to improved chironomid-temperature, and other, transfer functions.

[^1]: Data and code for this post are on [github](https://github.com/richardjtelford/zabinskie). For simplicity, all transfer functions are treated as if they are independent even though some are subsets of others.

Part of the variance in performance is explained by the choice of air or water temperatures as a calibration target [@Eggermont2011, Figure 1]. Models calibrated against water temperature have a worse performance even though chironomids spend most of their life in water. This is probably because water temperatures are spot measurements, and thus reflect recent weather, whereas air temperatures are climatological means derived from local weather stations or gridded climatologies [e.g., @Hijmans2005]. 

```{r calibrationTarget, fig.cap = "Figure 1. Effect of calibration target on RMSEP"}
ggplot(chiron, aes(x = AirWater, y = `RMSEP (°C)`, colour = AirWater)) + 
  geom_boxplot() + 
  geom_jitter(height = 0, width = 0.4) +
  xlab("Calibration target")
```
```{r airOnly}
chiron <- filter(chiron, AirWater == "air")
```

The following analyses concern only the `r nrow(chiron)` transfer functions calibrated against air temperatures. The range of RMSEPs is unchanged.  The coefficient of determination (r^2^) varies between `r min(chiron$r2jack)` and `r round(max(chiron$r2jack), 2)`.

I have several hypotheses that might explain the difference in performance between models:

 - Longer temperature gradients in the calibration set give larger RMSEPs but higher r^2^.
 - Large calibrations sets give better performance
 - Low-resolution taxonomy is associated with worse performance
 - Chironomid sensitivity to temperature varies with temperature
 - Some authors have followed the (perhaps unwise) advice to maximise the temperature gradient and minimise all other gradients better than others 
 - Some authors have been more zealous in removing outliers than others.
 - Authors have accidentally reported the apparent rather than cross-validated performance statistics

Only some of these can be answered with the data currently to hand.

## Gradient length

```{r}
library("broom")
mod <- lm(`RMSEP (°C)` ~ `Range of gradient (°C)`, data = chiron)
gmod <- glance(mod)

rmod <- lm(r2jack ~ `Range of gradient (°C)`, data = chiron)
grmod <- glance(rmod)

mod2 <- lm(`RMSEP (°C)` ~ `Range of gradient (°C)` + I(`Range of gradient (°C)`^2), data = chiron)
#anova(mod, mod2)

```

The temperatures in the calibration sets range from `r min(chiron$'Range of gradient (°C)')` to `r max(chiron$'Range of gradient (°C)')` °C. RMSEP has a strong (R^2^ = `r round(gmod$r.squared, 2)` ; p `r format_p(gmod$p.value)`) positive (slope = `r signif(coef(mod)[2], 2)` °C/°C) relationship with temperature range (Figure 2). The relation between temperature range and r^2^ is weaker (R^2^ = `r round(grmod$r.squared, 2)`; p `r format_p(grmod$p.value)`), and also positive (slope = `r signif(coef(rmod)[2], 2)` °C^-1^).

```{r gradientfig, fig.cap="Figure 2. RMSEP and r^2^ against calibration set temperature range"}
g1 <- ggplot(chiron, aes(x = `Range of gradient (°C)`, y = `RMSEP (°C)`, colour = AirWater)) + 
  geom_point(show.legend = FALSE) + 
  geom_smooth(method = "lm", formula = y ~ x, show.legend = FALSE)

g2 <- g1 + aes(x = `Range of gradient (°C)`, y = r2jack)

grid.arrange(g1, g2, ncol = 2)
```

The r^2^ increases with temperature range because, even though RMSEP increases, RMSEP as a proportion of temperature range decreases. 

But why does the RMSEP incease with gradient length? There are two possible explanations: 

 1. With a larger temperature range, it is possible for cross-validation predictions to be less constrained. If this is true, it would suggest that transfer functions with a small temperature range have artificially reduced uncertainties.
 2. Larger temperature ranges have a lower density of lakes per °C and therefore species optima are poorly constrained.

```{r lakeDensity, fig.cap="Figure 3. Density of lakes as a function of temperature range."}
g1 + aes(x = `Range of gradient (°C)`, y = No_lakes/`Range of gradient (°C)`)
mod_a <-  lm(I(No_lakes/`Range of gradient (°C)`) ~ `Range of gradient (°C)`, data = chiron, subset = No_lakes < 400) 
gmod_a <- glance(mod_a)
```

Omitting the anomalously large composite calibration set [@Fortin2015], there is a significant (p `r format_p(gmod_a$p.value)`) negative trend (slope = `r signif(coef(mod_a)[2], 2)` lakes/°C/°C).

```{r lakedensity}
mod_den <-  lm(`RMSEP (°C)` ~ I(No_lakes/`Range of gradient (°C)`), data = chiron, subset = No_lakes < 400) 
gmod_den <- glance(mod_den)

mod_no450 <-  lm(`RMSEP (°C)` ~ `Range of gradient (°C)`, data = chiron, subset = No_lakes < 400) 
gmod_no450 <- glance(mod_no450)

mod_den_r <-  lm(`RMSEP (°C)` ~ I(No_lakes/`Range of gradient (°C)`) + `Range of gradient (°C)`, data = chiron, subset = No_lakes < 400) 
gmod_den_r <- glance(mod_den_r)
```

Lake density is a less good predictor (R^2^ = `r round(gmod_den$r.squared, 2)`) of RMSEP than temperature range. So while it might be an important contributor to the effect of temperature range on performance, it does not explain the whole effect. A model including both lake density and temperature range is not significantly better than a model with just temperature range (p `format_p(anova(mod_den_r, mod_no450)$Pr[2])`).

Simulated species-environment data may help understand the relative importance of lake density and pure temperature range. 

## Number of lakes

The number of observations in a calibration set has a large impact on transfer-function performance [@Reavie2011]. Reavie and Juggins [-@Reavie2011] took large diatom-phosphorus calibration sets and estimated the performance of stratified-random subsets of observations. They found that there were substantial improvements in performance until calibration sets included 40-70 observations and smaller improvements thereafter. Transfer-function performance will increase because, with more observations, species optima are better estimated and so reconstructions are more accurate.

Translating this calibration set size from diatoms to chironomids is difficult as it will depend on the amount of turnover in the assemblages along the gradient of interest (which could be estimated with detrended constrained correspondence analysis) and the amount of noise inherent in the different proxies. Assuming the numbers can be used as-is, `r round(mean(chiron$No_lakes < 40)) * 100` % of calibration sets have fewer than 40 lakes, and `r round(mean(chiron$No_lakes < 70) * 100)` % have fewer than 70. Many of the calibration sets are small.

```{r NoLakeRMSEP, fig.cap = "Figure 4. RMSEP against the number of lakes in each calibration set, omitting the anomalously large calibration set."}
g1  %+% filter(chiron, No_lakes < 400) + 
  aes(x = No_lakes,  y = `RMSEP (°C)`) + 
  geom_vline(xintercept = c(40, 70)) + 
  xlab("Number of lakes")

mod_nlakes <- lm(`RMSEP (°C)` ~ No_lakes, data = chiron, subset = No_lakes < 400)
mod_nlakes2 <- lm(`RMSEP (°C)` ~ No_lakes + `Range of gradient (°C)`, data = chiron, subset = No_lakes < 400)
```

However, the relationship between the number of lakes and RMSEP is not significant (p `r format_p(anova(mod_nlakes)$Pr[1])`). Probably none of the calibration sets are small enough to suffer the full effect observed by Reavie and Juggins [-@Reavie2011].


## Taxonomic resolution

Chironomid taxonomy has been refined [@Brooks2007] since the first chironomid transfer functions. 
Heiri and Lotter [-@Heiri2010] showed that the performance of transfer functions was sensitive to the level of taxonomic precision by analysing a calibration set at low, intermediate or high taxonomic resolution: the RMSEP declined from 1.59 to 1.41°C. At low taxonomic resolution, multiple ecologically disparate species may be merged into a single generic or supra-generic morphotaxon and the combined optima might be misleading. With higher taxonomic resolution, optima are more appropriate but may be poorly defined as individual taxa will inevitably be rarer than the merged taxon, and there is an increased risk of misidentification [@Heiri2010; @Velle2010].

Some estimate of the level of taxonomic precision used in each calibration set could be derived from the taxonomic works cited, or by examining the species lists. Lacking the patience to do either, I am going to use the year of publication as a proxy for taxonomic precision, assuming that earlier works used less precise taxonomy. The main problem with this approach is that some calibration sets [e.g., @Fortin2015] include earlier calibration sets and use the lowest-common taxonomy.

The number of taxa could also be used as a proxy for taxonomic resolution, but would need to be adjusted for the size of the calibration set and the environmental range spanned.

```{r taxonResolution, fig.cap="Figure 5. RMSEP against year of publication"}
g1 + aes(x = year, y = `RMSEP (°C)`)

mod_tx <- lm(`RMSEP (°C)` ~ `Range of gradient (°C)` + year, data = chiron)
#summary(mod_tx)
```

After accounting for the temperature range in the calibration set, year is not a significant predictor of calibration set length (p = `r format_p(anova(mod, mod_tx)$Pr[2])`). This is perhaps not surprising given that the `r  round((1.59 - 1.41)/1.59 * 100, 1)` % improvement found by Heiri and Lotter [-@Heiri2010] is small relative to the `r round(max(chiron$'RMSEP (°C)')/min(chiron$'RMSEP (°C)'), 1)`-fold range in RMSEP.
 
## Chironomid sensitivity to temperature varies with temperature

It is possible that chironomid sensitivity to temperature is not constant along the temperature gradient, but that turnover is higher, and hence more precise reconstructions are possible, over some parts of the gradient. Following Rapoport's rule [@Stevens1989], we might expect niches to be smaller, and hence turnover higher, at higher temperatures.

The optimal test for this would probably include detrended constrained correspondence analyses over different parts of the temperature gradient in different calibration sets to see if the length of the first axis (a measure of turnover) changes in a consistent manner. Not having this information to hand, I test if the transfer function RMSEP is a function of the mid-point temperature of the calibration set.

There is a strong relationship between the mid-point temperature and RMSEP, but, because of the hard boundary at 0°C, there is also a strong correlation between mid-point temperature and temperature range (Pearson r = `r round(cor(chiron$'Range of gradient (°C)', chiron$midT), 2)`, p `r format_p(cor.test(chiron$'Range of gradient (°C)', chiron$midT)$p.value)`).

```{r midpoint, fig.cap="Figure 6. RMSEP against mid-point temperature and mid-point temperature against temperature range"}
g3 <- g1 + aes(x = midT, y = `RMSEP (°C)`) + xlab("Mid-point temperature (°C)")

g4 <- g1 + aes(x = `Range of gradient (°C)`, y = midT) + ylab("Mid-point temperature (°C)")
grid.arrange(g3, g4, ncol = 2)

mod_mp <- lm(`RMSEP (°C)` ~ `Range of gradient (°C)` + midT, data = chiron)
#summary(mod_mp)
```

After accounting for the temperature range, mid-point temperature has a positive (`r signif(coef(mod_mp)[3], 2)` °C/°C) but not statistically significant (p `r format_p(anova(mod, mod_mp)$Pr[2])`) effect on RMSEP.

## Calibration set design

Many authors aim to generate calibration sets that maximise the length of the environmental gradient and minimise other environmental gradients. This design will maximise transfer-function performance, however this performance will probably be over-estimated and down-core reconstructions will be more uncertain than the transfer function reports  due to the risk of non-analogue environmental conditions. 

If some authors have strived harder than others to generate calibration sets with a single strong gradient, or have selected a geographic extent with a large temperature gradient, this would affect the performance of transfer functions. It is not immediately obvious how to test this.

## Outlier removal

Many papers do not discuss outlier removal. Of those that do, Brooks and Birks [@Brooks2001] removed two glacially fed lakes that were outliers in the transfer function, presumably because the water temperature was much colder than expected given the air temperature. Wu et al 2015 [@Wu2015] remove seven outliers because their absolute residuals were >2 SD away from the observed temperature. The deleted lakes mainly have anomalous pH, conductivity or depth. 

Removing outliers will improve the transfer-function performance statistics. Wholesale removal, as sometimes practiced with testate-amoeba water-depth transfer functions [e.g., @Woodland1998 who remove 29 of 163 observations], is probably unwise as there is no guarantee that the improvement in cross-validation performance statistics will be reflected in better predictions down-core.


## Mis-reported statistics

Larocque-Tobler et al [-@Larocque-Tobler2015] reported the RMSEP of their Canadian-Polish transfer function as 1.3°C (Larocque-Tobler et al [-@Larocque-Tobler2015] use bootstrap cross-validation: the leave-one-out RMSEP would be slightly lower). In the corrigendum [@Larocque-Tobler2016] corrected this to 2.3°C, blaming the inclusion of nine lakes with low chironomid counts and some other errors. It seems unlikely that such errors would reduce the RMSEP so much (or even at all). Perhaps the authors accidentally reported the apparent RMSE rather than the cross-validated RMSEP. The [supplementary material](http://www.sciencedirect.com/science/MiamiMultiMediaURL/1-s2.0-S0277379115000086/1-s2.0-S0277379115000086-mmc1.xlsx/271861/html/S0277379115000086/2726ee3fb5f3d83e9bddc06a4cdfa5fa/mmc1.xlsx) to Larocque-Tobler et al [-@Larocque-Tobler2015] report the leave-one-out WAPLS-2 RMSEP to be 2.14°C, comparable with the bootstrapped value in the corrigendum. The bootstrapped WAPLS RMSEPs in the supplementary material are impossible as they are lower than the leave-one-out statistics (and the percent reduction in RMSEP is wrong). However, since the WAPLS-3 has a worse performance than WAPLS-2 these numbers cannot be the apparent performance which are guaranteed to improve with more components. The origin of the RMSEP reported by Larocque-Tobler et al [-@Larocque-Tobler2015], as with so much in that paper, remains a mystery (this supplementary table was added after the second review).

Larocque-Tobler et al [-@Larocque-Tobler2015] include data from two earlier calibration sets [@Larocque2006; @Larocque2008], which have an RMSEP of 1.17 and 1.67°C, respectively (Table 1). The decrease in performance between Larocque et al [@Larocque2006] and Larocque et al [@Larocque2008] can be explained by the large increase in temperature range in the latter calibration set. Why the performance should worsen again when the Polish lakes are included is unclear and should have been explored in Larocque-Tobler et al [-@Larocque-Tobler2015] as it undermines the rationale for merging the calibration sets.

It is possible that other transfer functions also misreport statistics. I will check those I have in hand.

Although the apparent performance statistics are over optimistic, and not a good guide to model performance, I would recommend their inclusion in papers as the difference between the apparent and cross-validated performance is a measure of possible over-fitting, and the presence of both sets of statistics give some assurance that the wrong numbers are not being reported.

## Conclusions

Much of the variance in chironomid-air temperature transfer-function performance can be explained by the temperature range in the calibration set. It is not clear how much of this is driven by the increased possibility for less constrained predictions with longer gradients and how much by reductions in lake density along the gradient. Calibration-set size and taxonomic precision appear to be of secondary importance. Other possible factors include the choice of the target temperature variable and errors in the temperature which will vary geographically; methodological choices such as square-root transformation of species data and transfer function model type; and count sums.

If I have missed any transfer functions, please let me know.


```{r, eval = FALSE}
g1 + aes(x = `RMSEP as % of gradient`, y = r2jack) 
```

## Table 1

Chironomid-based transfer functions for air temperature (T). Methods include weighted-averaging with inverse deshrinking (WAinv), partial least-squares (PLS), weighted-averaging partial least-squares, weighted modern analogue technique, and Bayesian methods. Numbers in brackets after the model indicate the number of components/analogues used. Listed are the coefficient of determination (r^2^) between the predicted and observed values, the root-mean-squared-error of prediction (RMSEP), RMSEP as % of the gradient, and maximum bias. All statistics are based on leave-one-out cross validation, except where noted otherwise. Most values from Eggermont and Heiri [-@Eggermont2011]

```{r}
knitr::kable(select(chiron, -AirWater, -(year:midT)))
```

## References